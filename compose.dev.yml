services:

  vectordb:
    extends:
      file: compose.yml
      service: vectordb
    ports:
      - 6333:6333
      - 6334:6334

  api:
    extends:
      file: compose.yml
      service: api
    ports:
      - 8000:80
    environment:
      - VECTORDB_URL=http://vectordb:6334/
      # - DEFAULT_LLM_MODEL=openai/gpt-5
      # - FORCE_REINDEX=true
      # - DEFAULT_LLM_MODEL=openrouter/openai/gpt-5
      # - DEFAULT_LLM_MODEL=openrouter/mistralai/mistral-large
      # - DEFAULT_LLM_MODEL=openrouter/anthropic/claude-sonnet-4.5
      # - DEFAULT_LLM_MODEL=groq/deepseek-r1-distill-llama-70b
      # - DEFAULT_LLM_MODEL=groq/llama-3.3-70b-versatile
      # - DEFAULT_LLM_MODEL=deepseek/deepseek-reasoner
      # - DEFAULT_LLM_MODEL=deepseek/deepseek-chat
      # - DEFAULT_LLM_MODEL=azure/Mistral-Large-2411
      # - DEFAULT_LLM_MODEL=azure/DeepSeek-R1
      # - DEFAULT_LLM_MODEL=together/meta-llama/Llama-3-70b-chat-hf
      # - DEFAULT_LLM_MODEL=openai/o3-mini
      # - DEFAULT_LLM_MODEL=openai/gpt-4o-mini
    entrypoint: ["uv", "run", "uvicorn", "src.sparql_llm.agent.main:app", "--host", "0.0.0.0", "--port", "80", "--reload", "--log-config", "logging.yml"]

  # In case we need a GPU-enabled workspace, e.g. for generating embeddings
  # workspace:
  #   image: ghcr.io/vemonet/gpu-workspace:main
  #   # Enable GPUs in this container:
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: 1
  #   # Shared memory size for the container
  #   shm_size: '8g'
  #   volumes:
  #   - ./:/app
  #   # - /tmp/fastembed_cache/models--qdrant--bge-large-en-v1.5-onnx/:/tmp/fastembed_cache/models--qdrant--bge-large-en-v1.5-onnx/
  #   environment:
  #     TZ: Europe/Amsterdam
